- 再炫目的技术归根结底都是基本模型与方法在具体领域问题上的组合，理解基本模型与方法，以及不同模型之间的内在联系才是掌握机器学习要义所在。
- **机器学习的任务**，就是基于已知数据构造概率模型，反过来再运用概率模型对未知数据进行预测与分析。但频率学派与贝叶斯学派对概率迥异的认知将机器学习一分为二，发展出两套完全不同的理论体系，统计学习和符号学习。
- # 统计机器学习
- 统计机器学习所讲的概率，表示的是事件发生频率的极限值。当重复试验的次数趋近于无穷大时，事件发生的频率会收敛到真实的概率之上。这种观点背后暗含了一个前提，那就是概率是一个确定的值，并不会受单次观察结果的影响。基于这个逻辑，我试图理解一下统计机器学习模型的理论思路：
	- 待估计的参数是固定不变的常量，这个固定的常量是多少，我们暂时不知道。
	  logseq.order-list-type:: number
	- 而数据是随机的变量，每个数据可看做是参数支配下一次独立重复试验的结果。如何通过有限的采样数据来估计总体的参数，便是统计学的核心任务。
	  logseq.order-list-type:: number
	- 统计学的思路是通过采样分布来求解，确定采样分布之后，参数估计便可以等效成一个最优化问题。最常用的方法便是[[最大似然估计]]。
	  logseq.order-list-type:: number
	- 由于数据是随机的，所以每使用一组不同的数据，找到的参数的估计值也会不同，但这与参数本身是固定的并不矛盾。
	  logseq.order-list-type:: number
	- 因为受噪声和干扰等因素的影响，数据并不能真实的反映参数，所得到的估计值与真实值之间的偏差是存在一个置信区间的。可以通过损失函数的数学期望，即风险，来衡量估计结果的精确程度。风险最小的估计值就是所要接近的参数值。
	  logseq.order-list-type:: number
	- 而要计算风险，需要已知数据的真实分布，而数据的真实分布又依赖于参数，这似乎就陷入了一个“先有鸡还是先有蛋”的悖论。
	  logseq.order-list-type:: number
	- 为了解决这个悖论，引入经验风险，用训练数据的分布替代真实分布，使得风险可以被计算。
	  logseq.order-list-type:: number
	- 最后便是通过最小化经验风险，来找出最优的参数。
	  logseq.order-list-type:: number
- 总结频率学派对概率、统计学和机器学习的认知：
	- 概率是随机事件发生频率的极限值；
	- 执行参数估计时，视参数为确定的值，视数据为随机变量；
	- 主要使用最大似然估计法，让数据在给定参数下的似然概率最大化；
	- 以经验风险最小化作为模型选择的准则。
-